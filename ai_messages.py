"""
AI Message Generator using Ollama
Generates random contextual messages about hydration
"""

import random
import os
import json
from datetime import datetime


class AIMessageGenerator:
    """Generates messages using Ollama or fallback to pre-written messages"""

    def __init__(self, personality_file: str = "personalities/default.txt"):
        self.personality_file = personality_file
        self.ollama_available = False

        # Get model from config
        from config import CONFIG
        self.ollama_model = CONFIG.get("ai_ollama_model", "llama3.2:1b")

        # Try to import and connect to Ollama
        print("[AI DEBUG] Tentando conectar ao Ollama...")
        try:
            import ollama
            self.ollama = ollama
            print("[AI DEBUG] OK - Biblioteca ollama importada")

            # Test connection
            print("[AI DEBUG] Testando conex√£o com ollama.list()...")
            models_response = self.ollama.list()

            # Extract model names (handle different response formats)
            model_names = []

            # Try to access models attribute or key
            if hasattr(models_response, 'models'):
                # Response is an object with models attribute
                for m in models_response.models:
                    # Try to get the model name from different attributes
                    if hasattr(m, 'model'):
                        model_names.append(m.model)
                    elif hasattr(m, 'name'):
                        model_names.append(m.name)
            elif isinstance(models_response, dict) and 'models' in models_response:
                # Response is a dict with models key
                for m in models_response['models']:
                    name = m.get('name') or m.get('model') or str(m)
                    model_names.append(name)

            print(f"[AI DEBUG] OK - Conectado! Modelos disponiveis: {model_names}")

            # Check if our model is available
            if self.ollama_model in model_names or any(self.ollama_model in name for name in model_names):
                print(f"[AI DEBUG] OK - Modelo {self.ollama_model} encontrado")
            else:
                print(f"[AI DEBUG] AVISO - Modelo {self.ollama_model} nao encontrado nos nomes exatos")
                print(f"[AI DEBUG] Modelos disponiveis para usar: {model_names}")
                if model_names:
                    print(f"[AI DEBUG] Vamos tentar usar mesmo assim...")

            self.ollama_available = True
            print("[AI] OK - Ollama disponivel - usando IA para mensagens")

        except ImportError as e:
            print(f"[AI DEBUG] ERRO de import: {e}")
            print("[AI DEBUG] Execute: pip install ollama")
            self.ollama_available = False

        except Exception as e:
            print(f"[AI DEBUG] ERRO ao conectar: {type(e).__name__}: {e}")
            print("[AI DEBUG] Verifique se Ollama est√° rodando:")
            print("[AI DEBUG]   Windows: Ollama deve estar rodando (√≠cone na bandeja)")
            print("[AI DEBUG]   Teste: ollama list (no terminal)")
            self.ollama_available = False

        # Load personality/instructions
        self.personality = self._load_personality()

        # Fallback messages (when Ollama not available)
        self.fallback_messages = self._load_fallback_messages()

    def _load_personality(self) -> str:
        """Load personality/instruction file for the AI"""
        # Ensure personalities directory exists
        os.makedirs("personalities", exist_ok=True)

        # Create default if doesn't exist
        if not os.path.exists(self.personality_file):
            default_personality = """Voc√™ √© um assistente divertido e sarc√°stico que ajuda o usu√°rio a se manter hidratado.

ESTILO:
- Use humor sarc√°stico mas amig√°vel
- Seja breve (m√°ximo 2 frases, prefira 1 frase)
- Varie entre encorajamento, sarcasmo leve, e curiosidades
- Use emojis ocasionalmente (mas n√£o exagere)
- Seja direto e descontra√≠do

CONTEXTO:
Voc√™ receber√° informa√ß√µes sobre:
- Quanto o usu√°rio j√° bebeu hoje
- Quanto falta para a meta
- H√° quanto tempo n√£o bebe √°gua

EXEMPLOS DE TOM:
- "Vai morrer desidratado em... brincadeira, bebe √°gua a√≠ üíß"
- "Quase l√°! Falta s√≥... tudo isso de novo üòÖ"
- "Parab√©ns por n√£o virar uma uva passa hoje!"
- "Seus rins agradecem essa hidrata√ß√£o"
- "H2O √© a parada mais importante depois do oxig√™nio, sabia?"

IMPORTANTE:
- N√ÉO use formata√ß√£o Markdown
- N√ÉO use aspas em volta da mensagem
- Retorne APENAS a mensagem, nada mais
- M√°ximo de 100 caracteres
"""
            with open(self.personality_file, 'w', encoding='utf-8') as f:
                f.write(default_personality)

        # Load personality
        try:
            with open(self.personality_file, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"[AI] Erro ao carregar personalidade: {e}")
            return "Voc√™ √© um assistente amig√°vel que ajuda com hidrata√ß√£o."

    def _load_fallback_messages(self) -> dict:
        """Load fallback messages for when Ollama is not available"""
        return {
            "low_progress": [
                "Bora come√ßar o dia hidratado! üíß",
                "Aquela meta n√£o vai se alcan√ßar sozinha...",
                "Seus rins est√£o mandando um al√¥ üëã",
                "Lembra da √∫ltima vez que voc√™ bebeu √°gua? Nem eu.",
                "√Ågua: melhor que caf√© (quase)",
            ],
            "medium_progress": [
                "T√° indo bem! Mas n√£o para agora üöÄ",
                "Metade do caminho, falta s√≥... a outra metade",
                "Continue assim que vira peixe üêü",
                "Progresso detectado! Parab√©ns por ter mem√≥ria",
                "Voc√™ est√° 60% √°gua, vamos pra 70%?",
            ],
            "high_progress": [
                "Quase l√°, campe√£o! üèÜ",
                "Seus rins est√£o fazendo uma festa",
                "Mais um pouquinho e voc√™ vira fonte",
                "Impressionante, voc√™ realmente bebe √°gua!",
                "Pr√≥ximo n√≠vel: ser patrocinado por marca de √°gua",
            ],
            "goal_reached": [
                "META BATIDA! Voc√™ √© incr√≠vel! üéâ",
                "Parab√©ns por n√£o virar uma uva passa!",
                "N√≠vel de hidrata√ß√£o: LEND√ÅRIO",
                "Seus rins mandaram um e-mail de agradecimento",
                "Achievement unlocked: Ser Humano Funcional üèÖ",
            ],
            "reminder": [
                "Psiu... j√° faz um tempo que n√£o bebe √°gua",
                "T√° esperando o qu√™? Uma notifica√ß√£o? Aqui est√° ela!",
                "Aquele momento perfeito pra beber √°gua",
                "Oi, sou sua consci√™ncia hidratada üëª",
                "Alerta de desidrata√ß√£o iminente!",
            ],
            "random": [
                "√Ågua √© vida, literalmente üí¶",
                "Dica: √°gua n√£o tem calorias!",
                "Sabia que o c√©rebro √© 75% √°gua?",
                "Beber √°gua > ler not√≠cias tristes",
                "Plot twist: voc√™ precisa de √°gua",
                "√Ågua: o combust√≠vel premium do corpo",
                "Hidrata√ß√£o: 10/10, recomendo",
            ]
        }

    def generate_message(self, ml_current: int, ml_goal: int, minutes_since_last: int) -> tuple:
        """
        Generate a contextual message based on current status

        Args:
            ml_current: Current water intake in ml
            ml_goal: Daily goal in ml
            minutes_since_last: Minutes since last drink

        Returns:
            Tuple of (message_string, message_type)
            message_type can be: "celebration", "achievement", "reminder", "normal", "funny"
        """
        percentage = (ml_current / ml_goal) * 100 if ml_goal > 0 else 0

        # Determine message type based on context
        message_type = self._determine_message_type(percentage, minutes_since_last)

        if self.ollama_available:
            message = self._generate_with_ollama(ml_current, ml_goal, percentage, minutes_since_last)
        else:
            message = self._generate_fallback(percentage, minutes_since_last)

        return message, message_type

    def _determine_message_type(self, percentage: float, minutes_since_last: int) -> str:
        """Determine the type of message based on context"""
        if percentage >= 100:
            return "celebration"
        elif percentage >= 50 and percentage < 55:  # Just hit 50%
            return "achievement"
        elif percentage >= 75 and percentage < 80:  # Just hit 75%
            return "achievement"
        elif minutes_since_last > 45:
            return "reminder"
        elif percentage > 70:
            return "normal"  # Close to goal, encouraging
        else:
            # Random chance of funny message
            import random
            if random.random() < 0.3:  # 30% chance
                return "funny"
            return "normal"

    def _generate_with_ollama(self, ml_current: int, ml_goal: int, percentage: float, minutes_since_last: int) -> str:
        """Generate message using Ollama"""
        try:
            print(f"[AI] Gerando mensagem com modelo: {self.ollama_model}")

            # Build context prompt
            context = f"""SITUA√á√ÉO ATUAL:
- J√° bebeu: {ml_current}ml
- Meta di√°ria: {ml_goal}ml
- Progresso: {percentage:.0f}%
- √öltima vez que bebeu: h√° {minutes_since_last} minutos

Gere UMA mensagem curta e divertida para o usu√°rio baseado na situa√ß√£o acima."""

            prompt = f"{self.personality}\n\n{context}"

            # Call Ollama
            response = self.ollama.generate(
                model=self.ollama_model,
                prompt=prompt,
                options={
                    "temperature": 0.9,  # More creative
                    "num_predict": 150,  # Max tokens (permite mensagens mais longas)
                }
            )

            message = response['response'].strip()

            # Clean up message
            message = message.replace('"', '').replace("'", "")
            if message.startswith('-'):
                message = message[1:].strip()

            # N√£o limita mais o tamanho - deixa o arquivo de personalidade controlar
            return message

        except Exception as e:
            print(f"[AI] Erro ao gerar mensagem com Ollama: {e}")
            # Fallback to pre-written messages
            return self._generate_fallback(percentage, minutes_since_last)

    def _generate_fallback(self, percentage: float, minutes_since_last: int) -> str:
        """Generate message from pre-written pool"""

        # Choose category based on context
        if percentage >= 100:
            category = "goal_reached"
        elif percentage >= 70:
            category = "high_progress"
        elif percentage >= 40:
            category = "medium_progress"
        elif minutes_since_last > 45:
            category = "reminder"
        elif percentage > 0:
            category = "low_progress"
        else:
            category = "random"

        # Pick random message from category
        messages = self.fallback_messages.get(category, self.fallback_messages["random"])
        return random.choice(messages)

    def reload_personality(self):
        """Reload personality file (useful after editing)"""
        self.personality = self._load_personality()
        print("[AI] Personalidade recarregada")


def test_generator():
    """Test the message generator"""
    print("=" * 50)
    print("AI Message Generator Test")
    print("=" * 50)

    generator = AIMessageGenerator()

    # Test scenarios
    scenarios = [
        (500, 2500, 15, "In√≠cio do dia"),
        (1200, 2500, 30, "Progresso m√©dio"),
        (2000, 2500, 20, "Quase l√°"),
        (2600, 2500, 10, "Meta batida!"),
        (800, 2500, 60, "Faz tempo que n√£o bebe"),
    ]

    print(f"\nOllama dispon√≠vel: {generator.ollama_available}")
    print("-" * 50)

    for ml_current, ml_goal, minutes, description in scenarios:
        print(f"\n{description}:")
        print(f"  Status: {ml_current}ml / {ml_goal}ml ({ml_current/ml_goal*100:.0f}%)")
        print(f"  √öltima vez: h√° {minutes} min")
        message = generator.generate_message(ml_current, ml_goal, minutes)
        print(f"  üí¨ \"{message}\"")

    print("\n" + "=" * 50)


if __name__ == "__main__":
    test_generator()
