# Como Trocar o Modelo de IA

## Modelos Dispon√≠veis no Seu Sistema:

Para ver quais modelos voc√™ tem instalados:

```bash
ollama list
```

Resultado no seu caso:
```
llama3.2:1b    - Modelo leve (1.3GB)
llama3.1:latest - Modelo melhor (4.9GB)
```

## Como Trocar:

### Op√ß√£o 1: Editar config.py

Abra `config.py` e mude:

```python
"ai_ollama_model": "llama3.1:latest",  # Era llama3.2:1b
```

### Op√ß√£o 2: Editar user_config.json

Abra `user_config.json` e adicione/mude:

```json
{
  "ai_ollama_model": "llama3.1:latest",
  ...
}
```

## Modelos Recomendados:

### Para Frases Curtas e R√°pidas:
- `llama3.2:1b` - Mais r√°pido, mais leve (1GB)
- `llama3.2:3b` - Bom equil√≠brio

### Para Mensagens Mais Criativas e Inteligentes:
- `llama3.1:latest` (8B) - Melhor qualidade (5GB)
- `llama3.1:8b` - Mesmo modelo
- `llama3:8b` - Vers√£o anterior

### Para M√°xima Qualidade (se tiver GPU/RAM):
- `llama3.1:70b` - Qualidade incr√≠vel, mas pesado
- `llama3:70b`

## Como Instalar Novos Modelos:

```bash
ollama pull llama3.1:8b
```

Ou qualquer outro modelo do catalogo: https://ollama.com/library

## Performance:

| Modelo | Tamanho | Velocidade | Qualidade | Uso |
|--------|---------|------------|-----------|-----|
| llama3.2:1b | 1.3GB | ‚ö°‚ö°‚ö° Muito r√°pido | ‚≠ê‚≠ê B√°sica | Frases simples |
| llama3.2:3b | 2GB | ‚ö°‚ö° R√°pido | ‚≠ê‚≠ê‚≠ê Boa | Recomendado |
| llama3.1:8b | 4.9GB | ‚ö° Normal | ‚≠ê‚≠ê‚≠ê‚≠ê √ìtima | Mensagens criativas |
| llama3.1:70b | 40GB+ | üê¢ Lento | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Perfeita | Apenas se tiver GPU potente |

## Testando:

Depois de mudar, teste:

```bash
python ai_messages.py
```

Vai mostrar qual modelo est√° sendo usado:
```
[AI] Gerando mensagem com modelo: llama3.1:latest
```

---

**Dica:** O modelo `llama3.1:latest` (8B) √© um √≥timo equil√≠brio entre qualidade e velocidade! üöÄ
